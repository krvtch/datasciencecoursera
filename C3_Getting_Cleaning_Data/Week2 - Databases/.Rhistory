write.table(rate_PHP, file=filename2, row.names = FALSE, sep = '\t')
today <- read.table(filename2, header = T, sep = "\t")
summ <- rbind(summ, today)
write.table(summ, file=filename1, row.names = FALSE, sep = '\t')
library(jsonlite)
rates <- fromJSON("https://api.eremit.com.my/EremitService.svc/GetExchangeRates")
rates_df <- subset(data.frame(rates), select = c(2,4,5))
System_Time <- Sys.time()
st_df <- data.frame(System_Time)
filename1 <- paste(getwd(),"/summ_PHP.txt",sep = "")
summ <- read.table(filename1, header = T, sep = "\t")
rate_PHP <- cbind(rates_df[9,],System_Time)
filename2 <- paste(getwd(),"/today_PHP.txt",sep = "")
write.table(rate_PHP, file=filename2, row.names = FALSE, sep = '\t')
today <- read.table(filename2, header = T, sep = "\t")
summ <- rbind(summ, today)
write.table(summ, file=filename1, row.names = FALSE, sep = '\t')
cube <- function(x,n){
x^3
}
cube(3)
x <- 1:10
if(x >5){
x<-0
}
f <- function(x) {
g <- function(y) {
y + z
}
z <- 4
x + g(x)
}
z <- 10
f(3)
x <- 5
y <- if(x < 3) {
NA
} else {
10
}
y
mywdir <- getwd()
mywdir
complete <- function(directory, id = 1:332) {
setwd(directory)
newdf <- data.frame(Date=as.Date(character()),sulfate=double(),nitrate=double(),ID=integer())
summdf <- data.frame(id=integer(), nobs=integer())
for(i in id) {
if(i>0&i<10) {filename <- paste("00",i,".csv",sep = "")}
else if(i>9&i<100) {filename <- paste("0",i,".csv",sep = "")}
else if(i>99&i<333){filename <- paste(i,".csv",sep = "")}
else {print("Error: file not found. Try values between 1-332.")
# how to end the function
}
currentdf <- read.csv(filename, header = T)
nobs <- nrow(currentdf[complete.cases(currentdf),])
id <- i
nobsdf <- data.frame(id,nobs)
summdf <- rbind(summdf,nobsdf)
}
summdf
}
complete(mywdir, 1)
complete(mywdir, c(2,4,8,10,12))
complete(mywdir, 30:25)
complete(mywdir, 3)
library(rJava)
Sys.setenv()
Sys.setenv(JAVA_HOME='C:\\Program Files (x86)\\Java\\jre1.8.0_131')
library(rJava)
Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jre1.8.0_131')
library(rJava)
library(xlsx)
download.file("https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FDATA.gov_NGAP.xlsx",destfile = "C:/Users/User/Desktop/Coursera/Course3_Data_Cleaning/Week1/naturalgas.xlsx")
dat <- read.xlsx("naturalgas.xslx", rowIndex = c(18:23), colIndex = c(7:15))
dat <- read.xlsx("naturalgas.xslx", rowIndex = c(18:23), colIndex = c(7:15), sheetIndex = 1)
dat <- read.xlsx("naturalgas", rowIndex = c(18:23), colIndex = c(7:15), sheetIndex = 1)
getwd()
setwd("C:/Users/User/Desktop/Coursera/Course3_Data_Cleaning/Week1")
dat <- read.xlsx("naturalgas.xlsx", rowIndex = c(18:23), colIndex = c(7:15), sheetIndex = 1)
dat
sum
sum(dat$Zip*dat$Ext,na.rm=T)
download.file("https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml", destfile = "C:/Users/User/Desktop/Coursera/Course3_Data_Cleaning/Week1/baltimore_resto.xml")
install.packages("XML")
library(XML)
fileURL <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml"
doc <- xmlTreeParse(fileURL, useInternalNodes = TRUE)
doc <- xmlTreeParse(fileURL)
doc <- xmlTreeParse("baltimore_resto.xml", useInternalNodes = TRUE)
doc
doc <- xmlTreeParse("baltimore_resto.xml", useInternalNodes = TRUE)
doc
rootNode <- xmlRoot(doc)
rootNode
xmlName(rootNode)
xpathSApply(rootNode,"//zipcode",xmlValue)
zipcodes <- xmlSApply(rootNode,xmlValue)
zipcodes
zipcodes <- xpathSApply(rootNode,"//zipcode",xmlValue)
zipcodes
countzip <- table(zipcodes)
countzip
countzip
str(countzip)
table()
tables()
install.packages("data.tables")
install.packages("data.table")
library(data.table)
tables()
countzip
download.file("https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv", destfile = "C:/Users/User/Desktop/Coursera/Course3_Data_Cleaning/Week1/idaho.xml")
download.file("https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv", destfile = "C:/Users/User/Desktop/Coursera/Course3_Data_Cleaning/Week1/idaho.csv")
DT <- fread("idaho.csv")
tables()
pwgtp15
DT
DT$pwgtp15
a <- Sys.time()
mean(DT[DT$SEX==1,]$pwgtp15); mean(DT[DT$SEX==2,]$pwgtp15)
b <- Sys.time()
DT[,mean(pwgtp15),by=SEX]
c <- Sys.time()
tapply(DT$pwgtp15,DT$SEX,mean)
d <- Sys.time()
rowMeans(DT)[DT$SEX==1]; rowMeans(DT)[DT$SEX==2]
e <- Sys.time()
mean(DT$pwgtp15,by=DT$SEX)
f <- Sys.time()
sapply(split(DT$pwgtp15,DT$SEX),mean)
g <- Sys.time()
b-a
c-b
d-c
e-d
f-e
g-f
min(b-a,c-b,d-c,e-d,f-e,g-f)
a <- Sys.time()
mean(DT[DT$SEX==1,]$pwgtp15); mean(DT[DT$SEX==2,]$pwgtp15)
b <- Sys.time()
DT[,mean(pwgtp15),by=SEX]
c <- Sys.time()
tapply(DT$pwgtp15,DT$SEX,mean)
d <- Sys.time()
rowMeans(DT)[DT$SEX==1]; rowMeans(DT)[DT$SEX==2]
e <- Sys.time()
mean(DT$pwgtp15,by=DT$SEX)
f <- Sys.time()
sapply(split(DT$pwgtp15,DT$SEX),mean)
g <- Sys.time()
maindf <- read.csv("data.csv")
property.1M <- maindf[maindf$VAL==24,]
property.1M
maindf
table(property.1M)
head(maindf)
valonly <- maindf$VAL
valonly
complete.cases(valonly)
valonly <- maindf$VAL[complete.cases(maindf$VAL)]
valonly
table(valonly)
str(maindf$FES)
View(maindf)
View(maindf)
table(maindf$FES)
summary(maindf$FES)
DT <- fread("idaho.csv")
DT$pwgtp15
a <- Sys.time()
mean(DT[DT$SEX==1,]$pwgtp15); mean(DT[DT$SEX==2,]$pwgtp15)
b <- Sys.time()
DT[,mean(pwgtp15),by=SEX]
c <- Sys.time()
tapply(DT$pwgtp15,DT$SEX,mean)
d <- Sys.time()
rowMeans(DT)[DT$SEX==1]; rowMeans(DT)[DT$SEX==2]
e <- Sys.time()
mean(DT$pwgtp15,by=DT$SEX) # incorrect, not categorized by m/f
f <- Sys.time()
sapply(split(DT$pwgtp15,DT$SEX),mean)
g <- Sys.time()
rowMeans(DT)[DT$SEX==1]
DT$SEX==1
rowMeans(DT)
a <- Sys.time()
mean(DT[DT$SEX==1,]$pwgtp15); mean(DT[DT$SEX==2,]$pwgtp15)
b <- Sys.time()
DT[,mean(pwgtp15),by=SEX]
c <- Sys.time()
tapply(DT$pwgtp15,DT$SEX,mean)
d <- Sys.time()
rowMeans(DT)[DT$SEX==1]; rowMeans(DT)[DT$SEX==2]
e <- Sys.time()
mean(DT$pwgtp15,by=DT$SEX) # incorrect, not categorized by m/f
f <- Sys.time()
sapply(split(DT$pwgtp15,DT$SEX),mean)
g <- Sys.time()
b-a
c-b
d-c
f-e
g-f
a <- system.time(DT[DT$SEX==1,]$pwgtp15); mean(DT[DT$SEX==2,]$pwgtp15)
b <- system.time(DT[,mean(pwgtp15),by=SEX])
c <- system.time(tapply(DT$pwgtp15,DT$SEX,mean))
d <- system.time(rowMeans(DT)[DT$SEX==1],
rowMeans(DT)[DT$SEX==2])
e <- system.time(mean(DT$pwgtp15,by=DT$SEX)) # incorrect, not categorized by m/f
f <- system.time(sapply(split(DT$pwgtp15,DT$SEX),mean))
a
b
c
f
a$user
a
str(a)
mean(DT$pwgtp15,by=DT$SEX)
DT[DT$SEX==1,]$pwgtp15
mean(DT[DT$SEX==2,]$pwgtp15
)
mean(DT[DT$SEX==2,]$pwgtp15)
DT[DT$SEX==1,]$pwgtp15)
DT[DT$SEX==1,]$pwgtp15
mean(DT[DT$SEX==1,]$pwgtp15)
mean(DT[DT$SEX==2,]$pwgtp15)
DT[,mean(pwgtp15),by=SEX]
tapply(DT$pwgtp15,DT$SEX,mean)
rowMeans(DT)[DT$SEX==1]
rowMeans(DT)
DT$SEX==1
mean(DT$pwgtp15,by=DT$SEX)
sapply(split(DT$pwgtp15,DT$SEX),mean)
a <- system.time(mean(DT[DT$SEX==1,]$pwgtp15), mean(DT[DT$SEX==2,]$pwgtp15))
b <- system.time(DT[,mean(pwgtp15),by=SEX])
c <- system.time(tapply(DT$pwgtp15,DT$SEX,mean))
f <- system.time(sapply(split(DT$pwgtp15,DT$SEX),mean))
a
b
c
f
source("http://bioconductor.org/biocLite.R")
biocLite("rhdf5")
library(rhdf5)
created <- h5createFile("example.h5")
created
created = h5createFile("example.h5")
created = h5createGroup("example.h5","foo")
created = h5createGroup("example.h5","baa")
created = h5createGroup("example.h5","foo/foobaa")
h5ls("example.h5")
example.h5
created
A = matrix(1:10, nr=5,nc=2)
h5write(A, "example.h5","foo/A")
h5ls("example.h5")
B = array(seq(0.1,2.0,by=0.1),dim=c(5,2,2))
attr(B,"scale") <- "liter"
h5write(B, "example.h5","foo/foobaa/B")
h5ls("example.h5")
df = data.frame(1L:5L,seq(0,1,length.out = 5), c("ab", "cde", "fghi", "a", "s"), stringsAsFactors = FALSE)
df
h5write(df,"example.h5","df")
h5ls("example.h5")
readA = h5read("example.h5", "foo/A")
readB = h5read("example.h5", "foo/foobaa/B")
readC = h5read("example.h5", "df")
readA
readB
readC
View(readC)
View(readC)
h5write(c(12,13,14),"example.h5","foo/A",index=list(1:3,1))
h5read("example.h5","foo/A")
h5read("example.h5","foo/A",index=list(1:3,1))
setwd("C:/Users/User/Desktop/Coursera/Course3_Data_Cleaning/Week2 - Databases")
con = url("http://scholar.google.com/citation?user=HI-I6C0AAAAJ&hl=en")
htmlCode = readLines(con)
con = url("http://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en")
htmlCode = readLines(con)
con = url("http://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en")
htmlCode = readLines(con)
con = url("https://scholar.google.com/citations?hl=en&user=HI-I6C0AAAAJ&view_op=list_works&sortby=title")
htmlCode = readLines(con)
con = url("https://scholar.google.com/citations?view_op=view_citation&hl=en&user=HI-I6C0AAAAJ&sortby=title&citation_for_view=HI-I6C0AAAAJ:2P1L_qKh6hAC")
htmlCode = readLines(con)
con = url("https://www.coursera.org/learn/data-cleaning/lecture/oKUph/reading-from-the-web")
htmlCode = readLines(con)
con = url("http://www.michaeljgrogan.com/")
htmlCode = readLines(con)
con = url("http://www.michaeljgrogan.com/")
htmlCode = readLines(con)
con = url("http://www.michaeljgrogan.com/")
htmlCode = readLines(con)
htmlCode
close(con)
substr(htmlCode, start = 1, stop = 1000)
con = url("http://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en")
htmlCode = readLines(con)
close(con)
htmlCode
library(XML)
url <- "http://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en"
html <- htmlTreeParse(url,useInternalNodes = T)
html
xpathSApply(html, "//title", xmlValue)
xpathSApply(html, "//td[@id='col-citedby']", xmlValue)
xpathSApply(html, "//td[@id='col-citedby']", xmlValue)
aa <- xpathSApply(html, "//td[@id='col-citedby']", xmlValue)
aa
xpathSApply(html, "//td[@id='col-citedby']", xmlValue)
html
xpathSApply(html, "//td[@id='col-Cited by']", xmlValue)
xpathSApply(html, "//td[@id='col-Citedby']", xmlValue)
url <- "http://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en"
html <- htmlTreeParse(url,useInternalNodes = T)
xpathSApply(html, "//title", xmlValue)
xpathSApply(html, "//td[@id='col-Citedby']", xmlValue)
install.packages("httr")
library(httr)
html2 = GET(url)
content2 = content(html2,as="text")
parsedHtml = htmlParse(content2,asText = T)
xpathSApply(parsedHtml, "//title", xmlValue)
xpathSApply(parsedHtml, "//td[@id='col-Citedby']", xmlValue)
xpathSApply(html, "//td[@id='col-citedby']", xmlValue)
xpathApply(html, "//td[@id='col-Citedby']", xmlValue)
xpathSApply(html, "//td[@id='col-Citedby']", xmlValue)
xpathSApply(parsedHtml, "//td[@id='gsc_a_ac']", xmlValue)
xpathSApply(parsedHtml, "//td[@id='gsc_a_ac']", xmlValue)
xpathSApply(parsedHtml, "//gsc_a_ac", xmlValue)
html <- htmlTreeParse(url,useInternal = T)
xpathSApply(html, "//title", xmlValue)
xpathSApply(html, "//td[@id='col-Citedby']", xmlValue)
root.Node <- xmlRoot(html)
root.Node
xmlName(root.Node)
doc <- xmlTreeParse(url,useInternalNodes = T)
doc
url <- "http://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en"
html <- htmlTreeParse(url,useInternalNodes = T)
# root.Node <- xmlRoot(html)
# xmlName(root.Node)
xpathSApply(html, "//title", xmlValue)
xpathSApply(html, "//td[@id='col-Citedby']", xmlValue)
xpathSApply(parsedHtml, "//td", xmlValue)
xpathSApply(parsedHtml, "//td[11]", xmlValue)
xpathSApply(parsedHtml, "//td[]", xmlValue)
xpathSApply(parsedHtml, "//td", xmlValue)
xpathSApply(parsedHtml, "//sd", xmlValue)
xpathSApply(parsedHtml, "//td", xmlValue)
xpathSApply(parsedHtml, "//td[@id='Citations']", xmlValue)
xpathSApply(parsedHtml, "//td[@id='col-citedby']", xmlValue)
parsedHtml
parsedHtml
xpathSApply(parsedHtml, "//style", xmlValue)
xpathSApply(parsedHtml, "//a", xmlValue)
xpathSApply(parsedHtml, "//td[a]", xmlValue)
xpathSApply(parsedHtml, "//td class=", xmlValue)
xpathSApply(parsedHtml, "//td[a]", xmlValue)
xpathSApply(parsedHtml, "//td[[a]", xmlValue)
xpathSApply(parsedHtml, "//td[[a]]", xmlValue)
xpathSApply(parsedHtml, "//td[a][gsc_a_c]", xmlValue)
xpathSApply(parsedHtml, "//td[a]", xmlValue)
xpathSApply(url, "//title", xmlValue)
html <- htmlTreeParse(url,useInternalNodes = T)
html
xpathSApply(html, "//title", xmlValue)
xpathSApply(html, "//td[@id='col-citedby']", xmlValue)
?htmlTreeParse
xpathSApply(parsedHtml, "//td[@id='col']", xmlValue)
xpathSApply(parsedHtml, "//td[a]", xmlValue)
xpathSApply(parsedHtml, "//td", xmlValue)
library(XML)
url <- "http://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en"
html <- ?htmlTreeParse(url,useInternalNodes = T)
# root.Node <- xmlRoot(html)
# xmlName(root.Node)
xpathSApply(html, "//title", xmlValue)
library(XML)
url <- "http://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en"
html <- ?htmlTreeParse(url,useInternalNodes = T)
xpathSApply(html, "//title", xmlValue)
url <- "http://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en"
html <- htmlTreeParse(url,useInternalNodes = T)
# root.Node <- xmlRoot(html)
# xmlName(root.Node)
xpathSApply(html, "//title", xmlValue)
xpathSApply(html, "//td[@id='col-citedby']", xmlValue)
xpathSApply(html, "//td[a.gsc_a_ac]", xmlValue)
xpathSApply(html, "//td[a//gsc_a_ac]", xmlValue)
xpathSApply(html, "//td[a]", xmlValue)
xpathSApply(html, "//td[@id='gsc_a_ac']", xmlValue)
xpathSApply(html, "//class", xmlValue)
xpathSApply(html, "//td/a", xmlValue)
xpathSApply(html, "//td/a/gsc_a_ac", xmlValue)
xpathSApply(html, "//td/a/", xmlValue)
xpathSApply(html, "//td/a", xmlValue)
xpathSApply(html, "//td[@class='gsc_a_c", xmlValue)
xpathSApply(html, "//td[@class='gsc_a_c'", xmlValue)
xpathSApply(html, "//td[@class='gsc_a_c']", xmlValue)
library(httr)
html2 = GET(url)
content2 = content(html2,as="text")
parsedHtml = htmlParse(content2,asText = T)
xpathSApply(parsedHtml, "//title", xmlValue)
xpathSApply(parsedHtml, "//td[@id='col-citedby']", xmlValue)
xpathSApply(parsedHtml, "//td[@class='gsc_a_c']", xmlValue)
pg1 = GET("http://httpbin.org/basic-auth/user/passwd")
pg1
pg2 = GET("http://httpbin.org/basic-auth/user/passwd", authenticate("user", "passwd"))
pg2
names(pg2)
names(parsedHtml)
names(url)
names(html)
names(html2)
google = handle("http://google.com")
google
pg1 = GET(handle = google,path = "/")
pg1
try <- handle(url)
try
try1 <- GET(handle = try,path = "/")
try1
pg2 = GET(handle = google,path = "search")
pg2
myapp = oauth_app("twitter", key = "yourConsumerKeyHere", secret = "yourCOnsumerSecretHere")
myapp
sig = sign_oauth1.0(myapp,token = "yourTokenHere", token_secret = "yourTokenSecretHere")
homeTL = GET("https://api.twitter.com/1.1/statuses/home_timeline.json", sig)
homeTL
json1 <- content(homeTL)
json2 = jsonlite::fromJSON(toJSON(json1))
library(jsonlite)
json2 = jsonlite::fromJSON(toJSON(json1))
json2[1,1:4]
json2
setwd("C:/Users/User/Desktop/Coursera/Course3_Data_Cleaning/Week2 - Databases")
download.file("https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv", destfile = "C:/Users/User/Desktop/Coursera/Course3_Data_Cleaning/Week2 - Databases/acs.csv")
acs <- read.csv("acs.csv")
acs
head(acs,10)
head(acs,1)
str(acs)
install.packages("sqldf")
library(sqldf)
acs <- read.csv("acs.csv")
str(acs)
sqldf("select pwgtp1 from acs where AGEP < 50")
head(sqldf("select pwgtp1 from acs where AGEP < 50"),10)
sqldf("select distinct AGEP from acs")
head(sqldf("select distinct AGEP from acs"),10)
url <- "http://biostat.jhsph.edu/~jleek/contact.html"
html <- htmlTreeParse(url,useInternalNodes = T)
xpathSApply(html, "//title", xmlValue)
html
close(url)
con = url("http://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en")
htmlCode = readLines(con)
close(con)
htmlCode
pagelines <- readLines(html)
close(con)
con = url("http://biostat.jhsph.edu/~jleek/contact.html")
htmlCode = readLines(con)
close(con)
htmlCode
nchar(htmlCode[10])
nchar(htmlCode[c(10,20,30,100)])
install.packages("RFortran")
library(RFortran)
download.file("https://d396qusza40orc.cloudfront.net/getdata%2Fwksst8110.for", destfile = "C:/Users/User/Desktop/Coursera/Course3_Data_Cleaning/Week2 - Databases/cpc_fortran.for")
?read.fortran
cpc <- read.fortran("cpc_fortran.for",as.is = T)
cpc <- read.fortran("cpc_fortran.for",as.is = T,format = rF9.d)
cpc <- read.fortran("cpc_fortran.for",as.is = T,format = "rF9.d")
cpc <- read.fortran("cpc_fortran.for",as.is = T,format = "rF1.d")
cpc <- read.fortran("cpc_fortran.for",as.is = T,format = "rA1.d")
url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fwksst8110.for"
lines <- readLines(url, n=10)
lines
lines <- readLines(url)
lines
?read.fwf
url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fwksst8110.for"
lines <- readLines(url, n=10)
lines
lines <- readLines("cpc_fortran.for", n=10)
lines
cpc <- read.fwf("cpc_fortran.for",skip = 4, widths = c(12,7,4,9,4,9,4))
cpc
read.fwf("cpc_fortran.for",skip = 4, widths = c(9,7,4,9,4,9,4))
read.fwf("cpc_fortran.for",skip = 4, widths = c(10,7,4,9,4,9,4))
read.fwf("cpc_fortran.for",skip = 4, widths = c(10,4,4,9,4,9,4))
read.fwf("cpc_fortran.for",skip = 4, widths = c(10,8,4,9,4,9,4))
read.fwf("cpc_fortran.for",skip = 4, widths = c(10,5,4,9,4,9,4))
read.fwf("cpc_fortran.for",skip = 4, widths = c(10,6,4,9,4,9,4))
readLines("cpc_fortran.for", n=10)
lines[4]
lines[5]
nchar(lines[5])
12+7+4+9+4+9+4+9+4
read.fwf("cpc_fortran.for",skip = 4, widths = c(12,9,4,9,4,9,4))
read.fwf("cpc_fortran.for",skip = 4, widths = c(10,9,4,9,4,9,4))
lines <- readLines("cpc_fortran.for", n=10) # there are 4 lines of headers not needed
nchar(lines[5]) # to know the width for each column
cpc <- read.fwf("cpc_fortran.for",skip = 4, widths = c(10,9,4,9,4,9,4))
cpc
sum(cpc[,4])
